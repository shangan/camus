#--------------------------------------------------------
# Needed Camus properties, more cleanup to come
# final top-level data output directory, sub-directory will be dynamically created for each topic pulled
#etl.destination.path=/user/hive/warehouse/test.db/
#etl.destination.path.topic.sub.dir=''
#-------------------------------------------------------

# HDFS location where you want to keep execution files, i.e. offsets, error logs, and count files
etl.execution.base.path=/camus/offsets/{camus.job.name}/
# where completed Camus job output directories are kept, usually a sub-dir in the base.path
etl.execution.history.path=/camus/offsets/{camus.job.name}/history
hdfs.default.classpath.dir=/camus/lib

#-----------------------------------------------------------
# Kafka-0.8 handles all zookeeper calls
#zookeeper.hosts=
#zookeeper.broker.topics=/brokers/topics
#zookeeper.broker.nodes=/brokers/ids
# Concrete implementation of the Encoder class to use (used by Kafka Audit, and thus optional for now)
#camus.message.encoder.class=com.linkedin.batch.etl.kafka.coders.DummyKafkaMessageEncoder
# Concrete implementation of the Decoder class to use
#------------------------------------------------------------
camus.message.decoder.class=com.meituan.camus.etl.SimpleStringMessageDecoder
# Used by avro-based Decoders to use as their Schema Registry
kafka.message.coder.schema.registry.class=com.linkedin.camus.example.DummySchemaRegistry
etl.record.writer.provider.class=com.linkedin.camus.etl.kafka.common.StringRecordWriterProvider
etl.output.record.delimiter=


# Used by the committer to arrange .avro files into a partitioned scheme. This will be the default partitioner for all
# topic that do not have a partitioner specified
etl.partitioner.class=com.meituan.camus.etl.kafka.coders.MeituanLogPartitioner
# Partitioners can also be set on a per-topic basis
#etl.partitioner.class.<topic-name>=com.your.custom.CustomPartitioner
# all files in this dir will be added to the distributed cache and placed on the classpath for hadoop tasks
# hdfs.default.classpath.dir=
# max hadoop tasks to use, each task can pull multiple topic partitions
mapred.map.tasks=48
# max historical time that will be pulled from each partition based on event timestamp
kafka.max.pull.hrs=1
# events with a timestamp older than this will be discarded.
kafka.max.historical.days=1
# Max minutes for each mapper to pull messages (-1 means no limit)
kafka.max.pull.minutes.per.task=-1

#-----------------------------------------------------------------------------------------------------
# if whitelist has values, only whitelisted topic are pulled.  if blacklist has values, nothing on the blacklist is pulled
# kafka.blacklist.topics=

# kafka.whitelist.topics=org.applog
#-----------------------------------------------------------------------------------------------------

# Name of the client as seen by kafka
kafka.client.name=camus
# Fetch Request Parameters
kafka.fetch.buffer.size=5242880
kafka.fetch.request.correlationid=
kafka.fetch.request.max.wait=1000
kafka.fetch.request.min.bytes=1024

#---------------------------------------------------------------------------------------------------
# Connection parameters.
kafka.brokers=rz-data-rt006:9092,rz-data-rt036:9092,rz-data-rt066:9092
#---------------------------------------------------------------------------------------------------

kafka.timeout.value=30000
#Stops the mapper from getting inundated with Decoder exceptions for the same topic
#Default value is set to 10
max.decoder.exceptions.to.print=5
#Controls the submitting of counts to Kafka
#Default value set to true
post.tracking.counts.to.kafka=true
log4j.configuration=false

#---------------------------------------------------------------------------------------------------------
# everything below this point can be ignored for the time being, will provide more documentation down the road
##########################
#etl.run.tracking.post=false
#kafka.monitor.tier=
#etl.counts.path=
#kafka.monitor.time.granularity=10
#etl.hourly=hourly
#etl.daily=daily
#etl.ignore.schema.errors=false
#----------------------------------------------------------------------------------------------------------
# configure output compression for deflate or snappy. Defaults to deflate
etl.output.codec=deflate
etl.deflate.level=6
#etl.output.codec=snappy
etl.default.timezone=Asia/Shanghai
etl.output.file.datetime.format=yyyy-MM-dd-HH-mm-ss
etl.output.file.time.partition.mins=60
etl.keep.count.files=false
etl.save.errors=false
etl.save.counts=false
etl.history.directory.limit=300
etl.execution.history.max.of.quota=.8


#------------------------------------------------------------------------------------------------
mapred.output.compress=false
mapred.output.compression.codec=com.hadoop.compression.lzo.LzopCodec
mapred.map.max.attempts=3
kafka.client.buffer.size=20971520
kafka.client.so.timeout=60000
#zookeeper.session.timeout=
#zookeeper.connection.timeout=
zookeeper.hosts=rz-data-rt-zk01:2181,rz-data-rt-zk02:2181,rz-data-rt-zk03:2181/camus

#camus.job.name=chenshangan-camusJob

monitor.zabbix.server=zabbix.lan.sankuai.com:10051
monitor.item.host=data-log1
zookeeper.base.path=/etl/
etl.fail.invalid.offset=true
mapreduce.job.queuename=root.hadoop-data.horilog
mapred.child.java.opts=-Dfile.encoding=utf-8\ -Xmx2048m


# SimpleStringMessageDecoder 赋给日志的时间为：系统当前时间 - delta_millis，可根据此特性调节没有时间戳日志的partition，默认值为3600000(即一个小时)，即放入上个小时
camus.message.delta.millis=3600000
# 此字段标识，当执行的时间是小时的多少分(默认60分钟，即不生效)之后，SimpleStringMessageDecoder 忽略camus.message.delta.millis，因为load日志需要在解析之前完成，过了解析时间应该放入当前时间分区
camus.message.delta.millis.ignore.min=60
# 输出表名可能不一定kakfa上的topic名称一致，运行用户自定义输出topic
#etl.target.topic
# 是否严格匹配whitelist中的topic名称(即通过equal匹配)，而不使用正则匹配；当配置的whitelist多于一个topic,或者kafka找不到对应的topic，会抛出NPE
kafka.whitelist.topic.strict=true
# etl指定是否全量重导
etl.reload=false
# 指定使用某次的execution结果作为重导起点，delta为小时差，查找此小时第一次执行的结果作为起点。每次执行都有一个execution，使用时间可避免execution的不确定性。
#etl.execution.delta.hour
# 指定delta的基准日志，格式'yyyymmdd'
#etl.execution.current.date
dfs.client.block.write.locateFollowingBlock.retries=10
etl.starttime.on=true
